<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PLATO:Generating Objects from Part Lists via Synthesized Layouts.">
  <meta name="keywords" content="Layout, Image Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PLATO:Generating Objects from Part Lists via Synthesized Layouts.</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  </head>
  <body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PLATO:</h1>
          <h2 class="title is-2 publication-title">Generating Objects from Part Lists via Synthesized Layouts.</h2>
          <div class="is-size-5">
            <span class="author-block">
              Amruta Muthal,                
            </span>
            <span class="author-block">
              Varghese P Kuruvilla,
            </span>
            <span class="author-block">
              Ravi Kiran Sarvadevabhatla,
            </span>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">International Institute of Information Technology, Hyderabad</span>
          </div>


          <!-- Add arxiv link here -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="Add arxiv link here" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/plato-gen/plato-gen" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <img id="teaser" width="150%" src="images/teaser_v5.png?03222242"> -->
      <img id="teaser" width="150%" src="images/plato_teaser_image.png">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman"><b>Figure 1. Shows synthesized layouts and corresponding objects generated by PLATO given an object category and a part
          list. Unlike traditional text-to-image (T2I) approaches, PLATO can generate diverse yet precisely controlled partial objects - e.g. a cat with only front legs visible.</b></p>
      </h2>
    </div>
  </div>
</section>


<section class="section"   style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Modern generative models often struggle to synthesize structured
            objects from detailed part specifications. They frequently produce
            anatomically implausible outputs or hallucinated components. We
            introduce PLATO, a novel two-stage framework that bridges this
            gap by enabling precise, part-controlled object generation. The first
            stage is PLayGen, our novel part layout generator which takes a list
            of parts and object category as input and synthesizes high fidelity
            layouts of part bounding boxes. To enhance PLayGen’s ability to learn inter-part relationships, we introduce novel structure-based
            loss functions. In the second stage, PLayGen’s synthesized layout is
            used to condition a custom-tuned ControlNet-style adapter, enforcing 
            spatial and connectivity constraints. This results in anatomically
            consistent, high-fidelity object generations containing precisely the
            user-specified parts. We further propose new part-level evaluation
            metrics to rigorously quantify adherence to part specifications. Extensive experiments 
            show that PLATO significantly outperforms
            state-of-the-art generative models and produces structurally coherent objects in a controllable manner — marking a step forward in
            modular, part-driven asset generation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>





<section class="section"   style="background-color:#e4e4f781">
  <div class="columns is-centered has-text-centered">
      <div style="width:100%;">
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3"><a href="" target="_blank"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/5886/5886212.png"> Demo -- Try it out  </a> </h2> 
          </div>
        </div>
        <!-- Abstract. -->
        <div style="float:middle; width:100%; border: 10px solid rgba(5, 130, 255, 0.534);">
          <h2 class="title is-5">PLATO Demo</h2>
          <div class="has-text-centered">
            <video autoplay loop muted playsinline style="max-width: 100%; height: auto;">
              <source src="images/acmm_plato_spedup.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            
          </div>          
        </div>
        <!--/ Abstract. -->

        <!-- <div style=" float:right; width:49.8%; border: 0px solid black;"> -->
          <!-- <h2 class="title is-5">II. Demo Instruction</h2> -->
          <!-- <div class="column is-five-fifths"> -->
            <!-- <div class="columns is-centered"> -->
            <!-- <div class="publication-video"> -->
              <!-- <iframe src="https://www.youtube.com/embed/-MCkU7IAGKs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
              <!-- <iframe src="https://user-images.githubusercontent.com/6631389/211468127-60dd5c69-8db9-43d2-a45d-e97b336e5249.mp4" frameborder="1" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
            <!-- </div> -->
          <!-- </div> -->
          <!-- </div> -->
        <!-- </div> -->
      </div>  
    </div>     
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Our Pipeline </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">     
            <img id="model" width="75%" src="images/plato_gif_images/QIF_pipeline_image.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>PLATO’s pipeline: PLayGen (PLATO's layout generator) generates a
                layout by taking an object and its associated part list as input.
                The generated layout is transformed into a control image
                which is fed to a custom tuned ControlNet to generate the final image.</b></p>
            </h3>   


        </div>
  </div>
</section>



<!-- <section class="section"   style="background-color:#e4e4f781">
  <div class="columns is-centered has-text-centered">
      <div style="width:90%;">

        <div style="float:left; width:47.1%; border: 0px solid black;">
          <h2 class="title is-5">I. Modulated Training</h2>
          <div class="content has-text-justified">
              Compared with other ways of using a pretrained diffusion model such as full-model finetuning, our newly added modulated layers are continual pre-trained on large grounding data (image-text-box) and is more cost-efficient. Just like Lego, one can plug and play different trained layers to enable different new capabilities.                 
          </div>  

          <div class="column is-five-fifths">
              <div class="columns is-centered">
                <img id="modulated_training" width="105%" src="images/approach_change.gif">
                
              </div>
          </div>
        </div>

        <div style=" float:right; width:47.8%; border: 0px solid black;">
          <h2 class="title is-5">II. Scheduled Sampling</h2>
          <div class="content has-text-justified">
              As a favorable property of our modulated training, GLIGEN supports scheduled sampling in the diffusion process for inference, where the model can dynamically choose to use grounding tokens (by adding the new layer) or original diffusion model with good prior (by kicking out the new layer), and thus balances generation quality and grounding ability.                   
          </div>  

          <div class="column is-five-fifths">
              <div class="columns is-centered">
                <img id="modulated_training" width="97%" src="images/approach_sampling.gif">
                
              </div>
          </div>
        </div>

      </div>  
    </div>     
</section> -->



<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/3515/3515174.png"> Results</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">


  <!-- Grounedtext2img. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><font color="#0000ff">Generations with variable part lists</font></h2>
      <img id="teaser" width="95%" src="images/plato_results.png">
      <h1>
        <p style="font-family:Times New Roman"><b>PLATO is able to correctly resolve left/right upper/lower placement for any given part list.</b>
      </h1>           
    </div>
  </div>
  <br>
  <br>
  <br>

  <!-- counterfactual. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><font color="#0000ff">PLATO's applications</font></h2>
      <img id="teaser" width="95%" src="images/plato_applications.png">
      <h1>
        <p style="font-family:Times New Roman"><b>PLATO's generations can be directly added to scenes since it uses an inpainting pipeline for image generation.</b>
      </h1>                 
    </div>
  </div>


  <br>
  <br>
  <br>

  <!-- counterfactual. -->
  <!-- <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><font color="#0000ff">Image Grounded T2I Generation (Bounding box)</font></h2>
      <img id="teaser" width="95%" src="images/image_condition.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN can also ground on reference images. Top row indicates reference images can provide more fine-grained details beyond text description such as style and shape or car. The second row shows reference image can also be used as style image in which case we find ground it into corner or edge of an image is sufficient.</b>
      </h1>                 
    </div>
  </div> -->


  <!-- <br>
  <br>
  <br> -->

  <!-- counterfactual. -->
  <!-- <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Grounded T2I Generation (Keypoints)</h2>
      <img id="teaser" width="95%" src="images/keypoint.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN can also ground human keypoints while doing text-to-image generation.</b>
      </h1>                 
    </div>
  </div> -->


  <!-- <br>
  <br>
  <br> -->

  <!-- counterfactual. -->
  <!-- <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><font color="#0000ff">Grounded Inpainting</font></h2>
      <img id="teaser" width="95%" src="images/inpaint.png">
      <h1>
        <p style="font-family:Times New Roman"><b>Like other diffusion models, GLIGEN can also perform grounded image inpaint, which can generate objects tightly following provided bounding boxes.</b>
      </h1>                 
    </div>
  </div> -->




  <!-- <br>
  <br>
  <br>

  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><font color="#0000ff">Canny Map Grounded T2I Generation</font></h2>
      <img id="teaser" width="98%" src="images/canny.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN results on canny maps.</b>
      </h1>                 
    </div>
  </div>


  <br>
  <br>
  <br>

  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><font color="#0000ff">Depth Map Grounded T2I Generation</font></h2>
      <img id="teaser" width="98%" src="images/depth.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN results on depth maps.</b>
      </h1>                 
    </div>
  </div> -->




  <!-- <br>
  <br>
  <br>

  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><font color="#0000ff">Normal Map Grounded T2I Generation</font></h2>
      <img id="teaser" width="98%" src="images/normal.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN results on normal maps.</b>
      </h1>                 
    </div>
  </div>


  <br>
  <br>
  <br> -->

  <!-- <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><font color="#0000ff">Semantic Map Grounded T2I Generation</font></h2>
      <img id="teaser" width="98%" src="images/sem.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN results on semantic maps.</b>
      </h1>                 
    </div>
  </div> -->




  <!-- <br>
  <br>
  <br>

  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><font color="#0000ff">Hed Map Grounded T2I Generation</font></h2>
      <img id="teaser" width="98%" src="images/hed.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN results on hed maps.</b>
      </h1>                 
    </div>
  </div>

  <br>
  <br>
  <br> -->

  <!-- <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Terms and Conditions</h2>
      <h1>
        <p style="font-family:Times New Roman">We have strict terms and conditions for using the model checkpoints and the demo; it is restricted to uses that follow the license agreement of <a href="https://github.com/CompVis/latent-diffusion">Latent Diffusion Model</a> and <a href="https://github.com/Stability-AI/StableDiffusion">Stable Diffusion</a>.
        </b>
      </h1>                 
    </div>     
  </div>



  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Broader Impact</h2>
      <h1>
        <p style="font-family:Times New Roman">It is important to note that our model GLIGEN is designed for open-world grounded text-to-image generation with caption and various condition inputs (e.g. bounding box). However, we also recognize the importance of responsible AI considerations and the need to clearly communicate the capabilities and limitations of our research. While the grounding ability generalizes well to novel spatial configuration and concepts, our model may not perform well in scenarios that are out of scope or beyond the intended use case. We strongly discourage the misuse of our model in scenarios, where our technology could be used to generate misleading or malicious images. We also acknowledge the potential biases that may be present in the data used to train our model, and the need for ongoing evaluation and improvement to address these concerns. To ensure transparency and accountability, we have included a model card that describes the intended use cases, limitations, and potential biases of our model. We encourage users to refer to this model card and exercise caution when applying our technology in new contexts. We hope that our work will inspire further research and discussion on the ethical implications of AI and the importance of transparency and accountability in the development of new technologies.</a>.
        </b>
      </h1>                 
    </div>     
  </div> -->



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
<!-- @article{li2023gligen,
  author      = {Li, Yuheng and Liu, Haotian and Wu, Qingyang and Mu, Fangzhou and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan and Lee, Yong Jae},
  title       = {GLIGEN: Open-Set Grounded Text-to-Image Generation},
  publisher   = {arXiv:2301.07093},
  year        = {2023},
} -->
</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a
      href="https://x-decoder-vl.github.io">X-Decoder</a>, licensed under a <a rel="license"
                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
    <p>

    <!-- <a href='https://github.com/Computer-Vision-in-the-Wild/'><img id="painting_icon" width="10%" src="https://avatars.githubusercontent.com/u/97258247?s=200&v=4">  -->
    <!-- </a>  -->
    <!-- <b> Related Links</b> :  -->
    <!-- <div class="content has-text-justified"> -->
      <!-- <ul> -->
        <!-- <li><a href='https://github.com/Computer-Vision-in-the-Wild/'>[Computer Vision in the Wild] </a> </li> -->
        <!-- <li>GLIGEN: (box, concept) &#8594 image || GLIP : image &#8594 (box, concept); See grounded image understanding in <a href='https://github.com/microsoft/GLIP'>[GLIP]</a></li> -->
        <!-- <li>Modulated design and training of foundation models for image understanding <a href='https://react-vl.github.io/'>[REACT]</a></li> -->
      <!-- </ul> -->
 <!--  -->
    <!-- </div>      -->

    
    </p>
  </div>
</section>


</body>
</html>
